<!doctype html>
<html lang="en" data-theme="light">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="css/pico.min.css">
    <title>Trustworthy Autonomous Systems</title>
  </head>
  <body>
    <main class="container">

		<article>
			<header>
				<hgroup>
					<h3>Attitudes and Perspectives on Trustworthy Autonomous Systems</h3>
					<h2>Networking project with a scoping study on top</h2>
				</hgroup>
			</header>
			
			
			
			<h3>Summary</h3>
			<p>
			As part of the <a href="https://www.tas.ac.uk/">Trustworthy Autonomous Systems (TAS)</a> hub, our long-term goal is to establish strategic partnerships for the purpose of studying trust and trustworthiness as they relate to autonomous systems, through the redaction of joint papers, the hosting of joint workshops, and other research activities. For this reason, we are travelling to academic and industry labs to meet with stakeholders at different levels involved in TAS-related topics. A side-objective of our visit is the production of a survey on attitudes and perspectives towards trustworthy autonomous systems-related issues and challenges. For this purpose, we use the <a href="https://lachlansresearch.com/the-moral-it-legal-it-decks/">Moral IT cards</a> - a tool <a href="https://www.horizon.ac.uk/project/moralit-enabling-design-of-ethical-and-legal-it-systems">developed in the Horizon research centre</a> to guide structured discussions about computer ethics.</p>
			
			<p>
			The main researchers involved in this survey are <strong>Dr Pepita Barnard</strong> and <strong>Dr Jeremie Clos</strong>. It is supported by the TAS hub (learn more about the TAS at <a href="https://www.tas.ac.uk/aboutus/overview/">tas.ac.uk</a>), a UK-wide research initiative created to research and advance trustworthiness in artificial intelligence and automated decision-making systems. </p>
			
			
			<p>The goal of our survey is two-fold:</p> 
			<ol>
				<li>Research approaches, challenges, and opportunities to trustworthy autonomous systems (e.g., human-centred/responsible/explainable/interpretable AI, Human-AI Interaction, Natural Language Processing etc.) in an academic and industrial context, with the dual objective of (1) building a survey study and (2) designing a workshop for the CHI 2023 conference to push a global research agenda.</li>
				<li>Discuss educational efforts on teaching curriculum development by identifying core topics and activities that reflect the multidisciplinary needs of the current and future <abbr title="Trustworthy Autonomous Systems">TAS</abbr> researchers and engineers.</li>
			</ol>
		
			
			
			<p>The rest of this page is structured as follows:</p>
			<ol>
				<li><a href="#questions">How the workshop is run</a></li>
				<li><a href="#who">Who is running the study</a></li>
				<li><a href="#faq">Frequently asked questions</a></li>
				<li><a href="#paperwork">Documentation of the project</a></li>
			</ol>
			


			<h3 id="questions">1. How the workshop is run</h3>
			<p>
			During the workshop, we will ask you to reflect on a set of questions, before starting the Moral IT cards exercise. Keep in mind that the process can be shortened of lengthened depending on time available.</p>

			<hgroup>
			<h4>Part 1. Questions</h4>
			<h4>Try to answer the following questions to the best of your knowledge.</h4>
			</hgroup>
			
			<ol>
			<br/><strong>Thinking about your own perspective and attitude on AI:</strong><br/><br/>
				<li>What does human-centred AI mean to you?  </li>
				<li>What does trustworthy autonomous systems mean to you? </li>
				<li>Field(s) do you work in / with? </li>
				<li>How does your work relate to autonomous systems / artificial intelligence? (your context) </li>
		
			
			<br/><strong>Thinking about a specific technology / technologies:</strong><br/><br/>
		
				<li>What technologies do you use or study? </li>
				<li>What existing assessment and appraisal tools, techniques, frameworks, or checklists do you already use or are aware of?
					<ul>
						<li>a.	What tools are used to assess / appraise AI / AS?</li>
						<li>b.	How is trustworthiness of AI / AS assessed / appraised â€“ tools used?</li>
					</ul>
				
				</li>
				<li>What do you think are overrated problems?</li>
				<li>What do you think are underrated problems?</li>
				<li>What do you think are the most important topics in the short term?</li>
				<li>What do you think are the most important topics in the long term?</li>
	
			
			<br/><strong>Thinking at a wider scale:</strong><br/><br/>
				<li>How do different countries approach the development, study, and use of human-centred autonomous systems (if you have a view on this)? </li>
				<li>How do they address and focus on making AI / AS trustworthy, so that they are used by people who can and do rightfully trust them.</li>
				<li>How does your country approach the development, study, and use of human-centred AI / AS (if you have a view on this)?</li>
				<li>Discussion of educational efforts on teaching curriculum development by identifying core topics and activities that reflect the multidisciplinary needs of the current and future TAS researchers and engineers.</li>
			</ol>
			
			<hgroup>
			<h4>Part 2. Moral IT cards exercise</h4>
			<h4>This exercise can be done with a Miro board or physical cards.</h4>
			</hgroup>
			
			<p>While thinking about an AI / AS technology:</p>
			
			<ol>
				<li>Look through the <a href="https://lachlansresearch.files.wordpress.com/2018/07/moral_it_full_deck_july_20181.pdf">Moral IT cards</a>, <a href="https://lachlansresearch.files.wordpress.com/2021/07/ace_cards.pdf">the Moral-IT card Aces</a>, and the <a href="https://lachlansresearch.files.wordpress.com/2018/07/process_board.pdf">Process Board</a></li>
				<li>Which cards are more important?
				<ul>
					<li>Go through each suit of cards, rank your most important cards w.r.t trustworthiness.</li>
				</ul>
				</li>
				<li>Pick cards that represent or reflect:</li>
				<ul>
					<li>risks, issues, and concerns of the technology in mind; </li>
					<li>benefits of the technology;</li>
					<li>safeguards to manage or mitigate the issues you have identified; </li>
					<li>blocks or challenges to applying the safeguards.</li>
				</ul>
				<li>Are there gaps somewhere in the deck?</li>
				<li>What Risks, Issues, Concerns, Safeguards (to manage or mitigate the risks, issues, and concerns) and Challenges to implementing Safeguards are not reflected in the cards?</li>
				<li>What would fill those gaps?</li>
				<li>Do other known tools address gaps identified?  </li>
			</ol>


			

			<h3 id="who">2. Who is running the study</h3>
			<p><strong>Pepita</strong> has a background in healthcare provision, clinical informatics and health services research. I am a human factors research fellow at Horizon Digital Economy Research Institute, School of Computer Science, University of Nottingham. In our cobot maker space, my AI projects involve humans with cobots, telepresence robots, other intelligent agents and devices for industrial and domestic settings. </p>

			<p><strong>Jeremie</strong> has a background in traditional computer science, information retrieval and machine learning. He is a research fellow in natural language processing in the TAS and a teaching associate in the School of Computer Science working on curriculum development for human-centred AI modules.
			</p>


			<h3 id="faq">3. Frequently asked questions</h3>
			
				<ul>
					<li><strong>What are you aiming to achieve?</strong></li>
					
					<p>We are interested in studying trustworthy artificial intelligence. However, trustworthiness is a multifaceted concept that is understood and tackled differently depending on the person you ask. Our aim is to have a deeper understanding of the core differences in how we understand trustworthiness by comparing researchers and practitioners from the US with researchers and practitioners from the UK.</p> 
					
					<li><strong>Who is involved?</strong></li>
					
					<p>The lead researcher involved in the study is Dr Pepita Barnard, who is accompanied by Dr Jeremie Clos.</p>
					
					<li><strong>What do you want to do during the meeting?</strong></li>
					
					<p>We would like to use the Moral IT cards as a framework for our conversation, eliciting your views on trustworthiness in artificial intelligence and autonomous systems in general.</p> 
					
					<li><strong>What is the purpose of the data collection?</strong></li>
					
					<p>Our plan is to use this data to write a survey paper on the diverse views on trustworthiness in artificial intelligence, and use this paper to spur a larger collaborative effort such as a conference workshop in a large interdisciplinary conference (e.g., CHI).</p> 
					
					<li><strong>What is the reason for recording?</strong></li> 
					
					<p>The recording helps us avoid copious notetaking while trying to talk. Recording for collaboration requires the agreement from those collaborators who are present. The study is extra, and it must be consented to in writing by each participant. The recording (or more accurately, the recording of those who consented) can also be used for the illustrative study.</p>		
				</ul>	
			

			<h3 id="paperwork">4. General paperwork</h3>
			
				
			
				<strong>Information sheet</strong>: the <a href="files/information-sheet.pdf">information sheet</a> will tell you more details about the benefits and risk of the research, how we use your data (during and after the study), and how to withdraw from the study.<br/>
				<strong>Privacy notice</strong>: the <a href="files/privacy-notice.pdf">privacy notice</a> will give you more details about how we manage and secure your data, as well as your rights under GDPR.<br/>
				<strong>Consent form</strong>: in order to participate you will also have to fill this <a href="files/consent-form.docx">consent form</a> and send it back to us.
				
			
			

			<footer>
			<h3>Contact us</h3>
			<p>
				While our travel is limited by COVID conditions, teaching duties, and budgetary concerns we are always looking for opportunities to meet new people. If you are interested in potential collaborations, please <a href="mailto:pepita.barnard@nottingham.ac.uk,jeremie.clos@nottingham.ac.uk?subject=About the IPoTAS Study">e-mail us</a>.
			</p>
	
			</footer>
		</article>
		
			
    </main>
  </body>
</html>

